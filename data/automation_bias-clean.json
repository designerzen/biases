{"title":"Automation bias","revision":960777784,"text":"<div class=\"mw-parser-output\"><table class=\"vertical-navbox nowraplinks hlist\" style=\"float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%\"><tbody><tr><td style=\"padding-top:0.4em;line-height:1.2em\">Part of <a href=\"https://wikipedia.com/wiki/Category:Automation\" title=\"Category:Automation\">a series</a> on</td></tr><tr><th style=\"padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em\"><a href=\"https://wikipedia.com/wiki/Outline_of_automation\" title=\"Outline of automation\">Automation</a></th></tr><tr><td style=\"padding:0.2em 0 0.4em\"><a href=\"https://wikipedia.com/wiki/File:Industrial_robots-transparent.gif\" class=\"image\"><img alt=\"Industrial robots-transparent.gif\" src=\"//upload.wikimedia.org/wikipedia/commons/6/6d/Industrial_robots-transparent.gif\" decoding=\"async\" width=\"122\" height=\"129\" data-file-width=\"122\" data-file-height=\"129\" /></a></td></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Robotics\" title=\"Robotics\">Robotics</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Cybernetics\" title=\"Cybernetics\">Cybernetics</a></li></ul>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\n<a href=\"https://wikipedia.com/wiki/Trade_fair\" title=\"Trade fair\">Trade shows</a></th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Asia_and_South_Pacific_Design_Automation_Conference\" title=\"Asia and South Pacific Design Automation Conference\">ASP-DAC</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Design_Automation_Conference\" title=\"Design Automation Conference\">DAC</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Design_Automation_and_Test_in_Europe\" title=\"Design Automation and Test in Europe\">DATE</a></li>\n<li><a href=\"https://wikipedia.com/wiki/International_Conference_on_Computer-Aided_Design\" title=\"International Conference on Computer-Aided Design\">ICCAD</a></li></ul>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\nAwards</th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<p><a href=\"https://wikipedia.com/wiki/IEEE_Robotics_and_Automation_Award\" title=\"IEEE Robotics and Automation Award\">IEEE Robotics and Automation Award</a>\n</p>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\n<a href=\"https://wikipedia.com/wiki/Robot\" title=\"Robot\">Robots</a></th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<p><a href=\"https://wikipedia.com/wiki/Industrial_robot\" title=\"Industrial robot\">Industrial robot</a>\n<br /><a href=\"https://wikipedia.com/wiki/Autonomous_research_robot\" title=\"Autonomous research robot\">Autonomous research robot</a>\n<br /><a href=\"https://wikipedia.com/wiki/Domestic_robot\" title=\"Domestic robot\">Domestic robot</a>\n</p>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\nGeneral purpose</th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<p><a href=\"https://wikipedia.com/wiki/Home_automation\" title=\"Home automation\">Home automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Banking_automation\" class=\"mw-redirect\" title=\"Banking automation\">Banking automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Laboratory_automation\" title=\"Laboratory automation\">Laboratory automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Integrated_library_system\" title=\"Integrated library system\">Integrated library system</a>\n<br /><a href=\"https://wikipedia.com/wiki/Broadcast_automation\" title=\"Broadcast automation\">Broadcast automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Console_automation\" class=\"mw-redirect\" title=\"Console automation\">Console automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Building_automation\" title=\"Building automation\">Building automation</a>\n</p>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\nSpecific purpose</th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<p><a href=\"https://wikipedia.com/wiki/Automated_attendant\" title=\"Automated attendant\">Automated attendant</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automated_guided_vehicle\" title=\"Automated guided vehicle\">Automated guided vehicle</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automated_highway_system\" class=\"mw-redirect\" title=\"Automated highway system\">Automated highway system</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automated_pool_cleaner\" title=\"Automated pool cleaner\">Automated pool cleaner</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automated_reasoning\" title=\"Automated reasoning\">Automated reasoning</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automated_teller_machine\" title=\"Automated teller machine\">Automated teller machine</a>\n<br /><a href=\"https://wikipedia.com/wiki/Automatic_painting_(robotic)\" class=\"mw-redirect\" title=\"Automatic painting (robotic)\">Automatic painting (robotic)</a>\n<br /><a href=\"https://wikipedia.com/wiki/Pop_music_automation\" title=\"Pop music automation\">Pop music automation</a>\n<br /><a href=\"https://wikipedia.com/wiki/Robotic_lawn_mower\" title=\"Robotic lawn mower\">Robotic lawn mower</a>\n<br /><a href=\"https://wikipedia.com/wiki/Telephone_switchboard\" title=\"Telephone switchboard\">Telephone switchboard</a>\n<br /><a href=\"https://wikipedia.com/wiki/Vending_machine\" title=\"Vending machine\">Vending machine</a>\n</p>\n<hr /></td>\n</tr><tr><th style=\"padding:0.1em\">\nSocial movements</th></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Technocracy_movement\" title=\"Technocracy movement\">Technocracy movement</a></li>\n<li><a href=\"https://wikipedia.com/wiki/The_Venus_Project\" class=\"mw-redirect\" title=\"The Venus Project\">Venus Project</a></li>\n<li><a href=\"https://wikipedia.com/wiki/The_Zeitgeist_Movement\" title=\"The Zeitgeist Movement\">Zeitgeist Movement</a></li></ul></td>\n</tr><tr><td style=\"text-align:right;font-size:115%\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"https://wikipedia.com/wiki/Template:Automation\" title=\"Template:Automation\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"https://wikipedia.com/w/index.php?title=Template_talk:Automation&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Template talk:Automation (page does not exist)\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Automation&amp;action=edit\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p><b>Automation bias</b> is the propensity for humans to favor suggestions from automated <a href=\"https://wikipedia.com/wiki/Decision_support_system\" title=\"Decision support system\">decision-making systems</a> and to ignore contradictory information made without automation, even if it is correct.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup> Automation bias stems from the <a href=\"https://wikipedia.com/wiki/Social_psychology\" title=\"Social psychology\">social psychology</a> literature that found a bias in human-human interaction that showed that people assign more positive evaluations to decisions made by humans than to a neutral object.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> The same type of positivity bias has been found for human-automation interaction,<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> where the automated decisions are rated more positively than neutral.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup> This has become a growing problem for decision making as <a href=\"https://wikipedia.com/wiki/Intensive_care_units\" class=\"mw-redirect\" title=\"Intensive care units\">intensive care units</a>, <a href=\"https://wikipedia.com/wiki/Nuclear_power_plants\" class=\"mw-redirect\" title=\"Nuclear power plants\">nuclear power plants</a>, and <a href=\"https://wikipedia.com/wiki/Cockpit\" title=\"Cockpit\">aircraft cockpits</a> have increasingly integrated computerized system monitors and decision aids to mostly factor out possible human error. Errors of automation bias tend to occur when decision-making is dependent on computers or other automated aids and the human is in an observatory role but able to make decisions. Examples of automation bias range from urgent matters like flying a plane on automatic pilot to such mundane matters as the use of <a href=\"https://wikipedia.com/wiki/Spell-checking\" class=\"mw-redirect\" title=\"Spell-checking\">spell-checking</a> programs.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-0\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup>\n</p>\n<div id=\"toc\" class=\"toc\" role=\"navigation\" aria-labelledby=\"mw-toc-heading\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2 id=\"mw-toc-heading\">Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Disuse_and_Misuse\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Disuse and Misuse</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Errors_of_commission_and_omission\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Errors of commission and omission</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Factors\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Factors</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Screen_design\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Screen design</span></a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Availability\"><span class=\"tocnumber\">3.2</span> <span class=\"toctext\">Availability</span></a></li>\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Awareness_of_process\"><span class=\"tocnumber\">3.3</span> <span class=\"toctext\">Awareness of process</span></a></li>\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Team_vs._individual\"><span class=\"tocnumber\">3.4</span> <span class=\"toctext\">Team vs. individual</span></a></li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Training\"><span class=\"tocnumber\">3.5</span> <span class=\"toctext\">Training</span></a></li>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Automation_failure_and_&quot;learned_carelessness&quot;\"><span class=\"tocnumber\">3.6</span> <span class=\"toctext\">Automation failure and \"learned carelessness\"</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Provision_of_system_confidence_information\"><span class=\"tocnumber\">3.7</span> <span class=\"toctext\">Provision of system confidence information</span></a></li>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#External_pressures\"><span class=\"tocnumber\">3.8</span> <span class=\"toctext\">External pressures</span></a></li>\n<li class=\"toclevel-2 tocsection-12\"><a href=\"#Definitional_problems\"><span class=\"tocnumber\">3.9</span> <span class=\"toctext\">Definitional problems</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#Automation-induced_complacency\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Automation-induced complacency</span></a></li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Sectors\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Sectors</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-15\"><a href=\"#Aviation\"><span class=\"tocnumber\">5.1</span> <span class=\"toctext\">Aviation</span></a></li>\n<li class=\"toclevel-2 tocsection-16\"><a href=\"#Health_care\"><span class=\"tocnumber\">5.2</span> <span class=\"toctext\">Health care</span></a></li>\n<li class=\"toclevel-2 tocsection-17\"><a href=\"#Military\"><span class=\"tocnumber\">5.3</span> <span class=\"toctext\">Military</span></a></li>\n<li class=\"toclevel-2 tocsection-18\"><a href=\"#Automotive\"><span class=\"tocnumber\">5.4</span> <span class=\"toctext\">Automotive</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-19\"><a href=\"#Correcting_bias\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Correcting bias</span></a></li>\n<li class=\"toclevel-1 tocsection-20\"><a href=\"#See_also\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-21\"><a href=\"#References\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-22\"><a href=\"#Further_reading\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">Further reading</span></a></li>\n<li class=\"toclevel-1 tocsection-23\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Disuse_and_Misuse\">Disuse and Misuse</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=1\" title=\"Edit section: Disuse and Misuse\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>An operator's trust in the system can also lead to different interactions with the system, including system use, misuse, disuse, and abuse.<sup id=\"cite_ref-ParasuramanRiley1997_6-0\" class=\"reference\"><a href=\"#cite_note-ParasuramanRiley1997-6\">&#91;6&#93;</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"https://wikipedia.com/wiki/Wikipedia:Vagueness\" title=\"Wikipedia:Vagueness\"><span title=\"This information is too vague. (December 2019)\">vague</span></a></i>&#93;</sup>\n</p><p>The tendency toward overreliance on automated aids is known as \"automation misuse\".<sup id=\"cite_ref-ParasuramanRiley1997_6-1\" class=\"reference\"><a href=\"#cite_note-ParasuramanRiley1997-6\">&#91;6&#93;</a></sup><sup id=\"cite_ref-international_Journal_of_Aviation_Psychology_7-0\" class=\"reference\"><a href=\"#cite_note-international_Journal_of_Aviation_Psychology-7\">&#91;7&#93;</a></sup> Misuse of automation can be seen when a user fails to properly monitor an automated system, or when the automated system is used when it should not be. This is in contrast to disuse, where the user does not properly utilize the automation either by turning it off or ignoring it. Both misuse and disuse can be problematic, but automation bias is directly related to misuse of the automation through either too much trust in the abilities of the system, or defaulting to using heuristics. Misuse can lead to lack of monitoring of the automated system or blind agreement with an automation suggestion, categorized by two types of errors, errors of omission and errors of commission, respectively.<sup id=\"cite_ref-:0_8-0\" class=\"reference\"><a href=\"#cite_note-:0-8\">&#91;8&#93;</a></sup><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup><sup id=\"cite_ref-ParasuramanRiley1997_6-2\" class=\"reference\"><a href=\"#cite_note-ParasuramanRiley1997-6\">&#91;6&#93;</a></sup>\n</p><p>Automation use and disuse can also influence stages of information processing: information acquisition, information analysis, decision making and action selection, and action implementation.<sup id=\"cite_ref-Wickens2015_10-0\" class=\"reference\"><a href=\"#cite_note-Wickens2015-10\">&#91;10&#93;</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"https://wikipedia.com/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\"><span title=\"This citation requires a reference to the specific page or range of pages in which the material appears. (December 2019)\">page&#160;needed</span></a></i>&#93;</sup>\n</p><p>For example, information acquisition, the first step in information processing, is the process by which a user registers input via the senses.<sup id=\"cite_ref-Wickens2015_10-1\" class=\"reference\"><a href=\"#cite_note-Wickens2015-10\">&#91;10&#93;</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"https://wikipedia.com/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\"><span title=\"This citation requires a reference to the specific page or range of pages in which the material appears. (December 2019)\">page&#160;needed</span></a></i>&#93;</sup> An automated engine gauge might assist the user with information acquisition through simple interface features—such as highlighting changes in the engine's performance—thereby directing the user's selective attention. When faced with issues originating from an aircraft, pilots may tend to overtrust an aircraft's engine gauges, losing sight of other possible malfunctions not related to the engine. This attitude is a form of automation complacency and misuse. If, however, the pilot devotes time to interpret the engine gauge, and manipulate the aircraft accordingly, only to discover that the flight turbulence has not changed, the pilot may be inclined to ignore future error recommendations conveyed by an engine gauge—a form of automation complacency leading to disuse.\n</p>\n<h2><span class=\"mw-headline\" id=\"Errors_of_commission_and_omission\">Errors of commission and omission</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=2\" title=\"Edit section: Errors of commission and omission\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Automation bias can take the form of commission errors, which occur when users follow an automated directive without taking into account other sources of information. Conversely, omission errors occur when automated devices fail to detect or indicate problems and the user does not notice because they are not properly monitoring the system.<sup id=\"cite_ref-Sagepub_11-0\" class=\"reference\"><a href=\"#cite_note-Sagepub-11\">&#91;11&#93;</a></sup>\n</p><p>Errors of omission have been shown to result from cognitive vigilance decrements, while errors of commission result from a combination of a failure to take information into account and an excessive trust in the reliability of automated aids.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-1\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup> Errors of commission occur for three reasons: (1) overt redirection of attention away from the automated aid; (2) diminished attention to the aid; (3) active discounting of information that counters the aid's recommendations.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-0\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup> Omission errors occur when the human decision-maker fails to notice an automation failure, either due to low vigilance or overtrust in the system.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-2\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup> For example, a spell-checking program incorrectly marking a word as misspelled and suggesting an alternative would be an error of commission, and a spell-checking program failing to notice a misspelled word would be an error of omission. In these cases, automation bias could be observed by a user accepting the alternative word without consulting a dictionary, or a user not noticing the incorrectly misspelled word and assuming all the words are correct without reviewing the words.\n</p><p>Training that focused on the reduction of automation bias and related problems has been shown to lower the rate of commission errors, but not of omission errors.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-3\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Factors\">Factors</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=3\" title=\"Edit section: Factors\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The presence of automatic aids, as one source puts it, \"diminishes the likelihood that decision makers will either make the cognitive effort to seek other diagnostic information or process all available information in cognitively complex ways.\" It also renders users more likely to conclude their assessment of a situation too hastily after being prompted by an automatic aid to take a specific course of action.<sup id=\"cite_ref-international_Journal_of_Aviation_Psychology_7-1\" class=\"reference\"><a href=\"#cite_note-international_Journal_of_Aviation_Psychology-7\">&#91;7&#93;</a></sup>\n</p><p>According to one source, there are three main factors that lead to automation bias. First, the human tendency to choose the least cognitive approach to decision-making, which is called the <a href=\"https://wikipedia.com/wiki/Cognitive_miser\" title=\"Cognitive miser\">cognitive miser</a> hypothesis. Second, the tendency of humans to view automated aids as having an analytical ability superior to their own. Third, the tendency of humans to reduce their own effort when sharing tasks, either with another person or with an automated aid.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-1\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>Other factors leading to an over-reliance on automation and thus to automation bias include inexperience in a task (though inexperienced users tend to be most benefited by automated decision support systems), lack of confidence in one's own abilities, a lack of readily available alternative information, or desire to save time and effort on complex tasks or high workloads.<sup id=\"cite_ref-goddard2012_13-0\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup><sup id=\"cite_ref-Alberdi2009_14-0\" class=\"reference\"><a href=\"#cite_note-Alberdi2009-14\">&#91;14&#93;</a></sup><sup id=\"cite_ref-goddard2014_15-0\" class=\"reference\"><a href=\"#cite_note-goddard2014-15\">&#91;15&#93;</a></sup><sup id=\"cite_ref-:0_8-1\" class=\"reference\"><a href=\"#cite_note-:0-8\">&#91;8&#93;</a></sup> It has been shown that people who have greater confidence in their own decision-making abilities tend to be less reliant on external automated support, while those with more trust in decision support systems (DSS) were more dependent upon it.<sup id=\"cite_ref-goddard2012_13-1\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Screen_design\">Screen design</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=4\" title=\"Edit section: Screen design\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>One study, published in the <i><a href=\"https://wikipedia.com/wiki/Journal_of_the_American_Medical_Informatics_Association\" title=\"Journal of the American Medical Informatics Association\">Journal of the American Medical Informatics Association</a></i>, found that the position and prominence of advice on a screen can impact the likelihood of automation bias, with prominently displayed advice, correct or not, is more likely to be followed; another study, however, seemed to discount the importance of this factor.<sup id=\"cite_ref-goddard2012_13-2\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> According to another study, a greater amount of on-screen detail can make users less \"conservative\" and thus increase the likelihood of automation bias.<sup id=\"cite_ref-goddard2012_13-3\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> One study showed that making individuals accountable for their performance or the accuracy of their decisions reduced automation bias.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-4\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Availability\">Availability</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=5\" title=\"Edit section: Availability\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>\"The availability of automated decision aids,\" states one study by <a href=\"https://wikipedia.com/wiki/Linda_Skitka\" title=\"Linda Skitka\">Linda Skitka</a>, \"can sometimes feed into the general human tendency to travel the road of least cognitive effort.\"<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-5\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Awareness_of_process\">Awareness of process</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=6\" title=\"Edit section: Awareness of process\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>One study also found that when users are made aware of the reasoning process employed by a decision support system, they are likely to adjust their reliance accordingly, thus reducing automation bias.<sup id=\"cite_ref-goddard2012_13-4\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Team_vs._individual\">Team vs. individual</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=7\" title=\"Edit section: Team vs. individual\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The performance of jobs by crews instead of individuals acting alone does not necessarily eliminate automation bias.<sup id=\"cite_ref-The_International_Journal_of_Aviation_Psychology_16-0\" class=\"reference\"><a href=\"#cite_note-The_International_Journal_of_Aviation_Psychology-16\">&#91;16&#93;</a></sup><sup id=\"cite_ref-Sagepub_11-1\" class=\"reference\"><a href=\"#cite_note-Sagepub-11\">&#91;11&#93;</a></sup> One study has shown that when automated devices failed to detect system irregularities, teams were no more successful than solo performers at responding to those irregularities.<sup id=\"cite_ref-University_of_Illinois_at_Chicago_5-6\" class=\"reference\"><a href=\"#cite_note-University_of_Illinois_at_Chicago-5\">&#91;5&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Training\">Training</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=8\" title=\"Edit section: Training\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Training that focuses on automation bias in <a href=\"https://wikipedia.com/wiki/Aviation\" title=\"Aviation\">aviation</a> has succeeded in reducing omission errors by student pilots.<sup id=\"cite_ref-The_International_Journal_of_Aviation_Psychology_16-1\" class=\"reference\"><a href=\"#cite_note-The_International_Journal_of_Aviation_Psychology-16\">&#91;16&#93;</a></sup><sup id=\"cite_ref-Sagepub_11-2\" class=\"reference\"><a href=\"#cite_note-Sagepub-11\">&#91;11&#93;</a></sup>\n</p>\n<h3><span id=\"Automation_failure_and_.22learned_carelessness.22\"></span><span class=\"mw-headline\" id=\"Automation_failure_and_&quot;learned_carelessness&quot;\">Automation failure and \"learned carelessness\"</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=9\" title=\"Edit section: Automation failure and &quot;learned carelessness&quot;\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>It has been shown that automation failure is followed by a drop in operator trust, which in turn is succeeded by a slow recovery of trust. The decline in trust after an initial automation failure has been described as the first-failure effect.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-2\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup> By the same token, if automated aids prove to be highly reliable over time, the result is likely to be a heightened level of automation bias. This is called \"learned carelessness.\"<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-3\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Provision_of_system_confidence_information\">Provision of system confidence information</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=10\" title=\"Edit section: Provision of system confidence information\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>In cases where system confidence information is provided to users, that information itself can become a factor in automation bias.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-4\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"External_pressures\">External pressures</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=11\" title=\"Edit section: External pressures\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Studies have shown that the more external pressures are exerted on an individual's cognitive capacity, the more he or she may rely on external support.<sup id=\"cite_ref-goddard2012_13-5\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Definitional_problems\">Definitional problems</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=12\" title=\"Edit section: Definitional problems\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Although automation bias has been the subject of many studies, there continue to be complaints that automation bias remains ill-defined and that reporting of incidents involving automation bias is unsystematic.<sup id=\"cite_ref-goddard2012_13-6\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup><sup id=\"cite_ref-:0_8-2\" class=\"reference\"><a href=\"#cite_note-:0-8\">&#91;8&#93;</a></sup>\n</p><p>A review of various automation bias studies categorized the different types of tasks where automated aids were used as well as what function the automated aids served. Tasks where automated aids were used were categorized as monitoring tasks, diagnosis tasks, or treatment tasks. Types of automated assistance were listed as Alerting automation, which track important changes and alert the user, Decision support automation, which may provide a diagnosis or recommendation, or Implementation automation, where the automated aid performs a specified task.<sup id=\"cite_ref-:0_8-3\" class=\"reference\"><a href=\"#cite_note-:0-8\">&#91;8&#93;</a></sup> \n</p>\n<h2><span class=\"mw-headline\" id=\"Automation-induced_complacency\">Automation-induced complacency</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=13\" title=\"Edit section: Automation-induced complacency\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The concept of automation bias is viewed as overlapping with automation-induced complacency, also known more simply as automation complacency. Like automation bias, it is a consequence of the misuse of automation and involves problems of attention. While automation bias involves a tendency to trust decision-support systems, automation complacency involves insufficient attention to and monitoring of automation output, usually because that output is viewed as reliable.<sup id=\"cite_ref-goddard2012_13-7\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> \"Although the  concepts  of complacency  and automation bias have been discussed separately as if they were independent,\" writes one expert, \"they share several commonalities, suggesting they reflect different aspects of the same  kind of automation misuse.\" It has been proposed, indeed, that the concepts of complacency and automation bias be combined into a single \"integrative concept\" because these two concepts \"might represent different manifestations of overlapping automation-induced phenomena\" and because \"automation-induced complacency and automation bias represent closely linked theoretical concepts that show considerable overlap with respect to the underlying processes.\"<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-5\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>Automation complacency has been defined as \"poorer detection of system malfunctions under automation compared with under manual control.\" <a href=\"https://wikipedia.com/wiki/NASA\" title=\"NASA\">NASA's</a> <a href=\"https://wikipedia.com/wiki/Aviation_Safety_Reporting_System\" title=\"Aviation Safety Reporting System\">Aviation Safety Reporting System</a> (ASRS) defines complacency as \"self-satisfaction that  may result in non-vigilance based on an unjustified assumption of satisfactory system state.\" Several studies have indicated that it occurs most often when operators are engaged in both manual and automated tasks at the same time. In turn, the operators' perceptions of the automated system's reliability can influence the way in which the operator interacts with the system. Endsley (2017) describes how high system reliability can lead users to disengage from monitoring systems, thereby increasing monitoring errors, decreasing situational awareness, and interfering with an operator's ability to re-assume control of the system in the event performance limitations have been exceeded.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup> This complacency can be sharply reduced when automation reliability varies over time instead of remaining constant, but is not reduced by experience and practice. Both expert and inexpert participants can exhibit automation bias as well as automation complacency. Neither of these problems can be easily overcome by training.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-6\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>The term \"automation complacency\" was first used in connection with aviation accidents or incidents in which <a href=\"https://wikipedia.com/wiki/Aircraft_pilot\" title=\"Aircraft pilot\">pilots</a>, <a href=\"https://wikipedia.com/wiki/Air_traffic_controller\" title=\"Air traffic controller\">air-traffic controllers</a>, or  other workers failed to check systems sufficiently, assuming that everything was fine when, in reality, an accident was about to occur. Operator complacency, whether or not automation-related, has long been recognized as a leading factor in air accidents.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-7\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>As such, perceptions of reliability, in general, can result in a form of automation irony, in which more automation can decrease cognitive workload but increase the opportunity for monitoring errors. In contrast, low automation can increase workload but decrease the opportunity for monitoring errors.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup> Take, for example, a pilot flying through inclement weather, in which continuous thunder interferes with the pilot's ability to understand information transmitted by an air traffic controller (ATC). Despite how much effort is allocated to understanding information transmitted by ATC, the pilot's performance is limited by the source of information needed for the task. The pilot therefore has to rely on automated gauges in the cockpit to understand flight path information. If the pilot perceives the automated gauges to be highly reliable, the amount of effort needed to understand ATC and automated gauges may decrease. Moreover, if the automated gauges are perceived to be highly reliable, the pilot may ignore those gauges to devote mental resources for deciphering information transmitted by ATC. In so doing, the pilot becomes a complacent monitor, thereby running the risk of missing critical information conveyed by the automated gauges. If, however, the pilot perceives the automated gauges to be unreliable, the pilot will now have to interpret information from ATC and automated gauges simultaneously. This creates scenarios in which the operator may be expending unnecessary cognitive resources when the automation is in fact reliable, but also increasing the odds of identifying potential errors in the weather gauges should they occur. To calibrate the pilot's perception of reliability, automation should be designed to maintain workload at appropriate levels while also ensuring the operator remains engaged with monitoring tasks. The operator should be less likely to disengage from monitoring when the system's reliability can change as compared to a system that has consistent reliability (Parasuraman, 1993).<sup id=\"cite_ref-ParasuramanMolloy1993_19-0\" class=\"reference\"><a href=\"#cite_note-ParasuramanMolloy1993-19\">&#91;19&#93;</a></sup>\n</p><p>To some degree, user complacency offsets the benefits of automation, and when an automated system's reliability level falls below a certain level, then automation will no longer be a net asset. One 2007 study suggested that this automation occurs when the reliability level reaches approximately 70%. Other studies have found that automation with a reliability level below 70% can be of use to persons with access to the raw information sources, which can be combined with the automation output to improve performance.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-8\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Sectors\">Sectors</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=14\" title=\"Edit section: Sectors\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Automation bias has been examined across many research fields.<sup id=\"cite_ref-goddard2012_13-8\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> It can be a particularly major concern in aviation, <a href=\"https://wikipedia.com/wiki/Medicine\" title=\"Medicine\">medicine</a>, <a href=\"https://wikipedia.com/wiki/Process_control\" title=\"Process control\">process control</a>, and <a href=\"https://wikipedia.com/wiki/Military\" title=\"Military\">military</a> <a href=\"https://wikipedia.com/wiki/Command-and-control\" class=\"mw-redirect\" title=\"Command-and-control\">command-and-control</a> operations.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-9\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Aviation\">Aviation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=15\" title=\"Edit section: Aviation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>At first, discussion of automation bias focused largely on aviation. Automated aids have played an increasing role in cockpits, taking a growing role in the control of such flight tasks as determining the most fuel-efficient routes, navigating, and detecting and diagnosing system malfunctions. The use of these aids, however, can lead to less attentive and less vigilant information seeking and processing on the part of human beings. In some cases, human beings may place more confidence in the misinformation provided by flight computers than in their own skills.<sup id=\"cite_ref-international_Journal_of_Aviation_Psychology_7-2\" class=\"reference\"><a href=\"#cite_note-international_Journal_of_Aviation_Psychology-7\">&#91;7&#93;</a></sup>\n</p><p>An important factor in aviation-related automation bias is the degree to which pilots perceive themselves as responsible for the tasks being carried out by automated aids. One study of pilots showed that the presence of a second crewmember in the cockpit did not affect automation bias. A 1994 study compared the impact of low and high levels of automation (LOA) on pilot performance, and concluded that pilots working with a high LOA spent less time reflecting independently on flight decisions.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-10\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>In another study, all of the pilots given false automated alerts that instructed them to shut off an engine did so, even though those same pilots insisted in an interview that they would not respond to such an alert by shutting down an engine, and would instead have reduced the power to idle. One 1998 study found that pilots with approximately 440 hours of flight experience detected more automation failures than did nonpilots, although both groups showed complacency effects. A 2001 study of pilots using a cockpit automation system, the <a href=\"https://wikipedia.com/wiki/Engine-indicating_and_crew-alerting_system\" title=\"Engine-indicating and crew-alerting system\">Engine-indicating and crew-alerting system</a> (EICAS), showed evidence of complacency. The pilots detected fewer engine malfunctions when using the system than when performing the task manually.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-11\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>In a 2005 study, experienced air-traffic controllers used high-fidelity simulation of an <a href=\"https://wikipedia.com/wiki/Air_traffic_control\" title=\"Air traffic control\">ATC</a> (Free Flight) scenario that involved the detection of conflicts among \"self-separating\" aircraft. They had access to an automated device that identified potential conflicts several minutes ahead of time. When the device failed near the end of the simulation process, considerably fewer controllers detected the conflict than when the situation was handled manually. Other studies have produced similar findings.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-12\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>Two studies of automation bias in aviation discovered a higher rate of commission errors than omission errors, while another aviation study found 55% omission rates and 0% commission rates.<sup id=\"cite_ref-goddard2012_13-9\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> Automation-related omissions errors are especially common during the cruise phase. When a <a href=\"https://wikipedia.com/wiki/China_Airlines\" title=\"China Airlines\">China Airlines</a> flight lost power in one engine, the autopilot attempted to correct for this problem by lowering the left wing, an action that hid the problem from the crew. When the autopilot was disengaged, the airplane rolled to the right and descended steeply, causing extensive damage. The 1983 shooting down of a Korean Airlines 747 over <a href=\"https://wikipedia.com/wiki/Soviet\" class=\"mw-redirect\" title=\"Soviet\">Soviet</a> airspace occurred because the Korean crew \"relied on automation that had been inappropriately set up, and they never checked their progress manually.\"<sup id=\"cite_ref-international_Journal_of_Aviation_Psychology_7-3\" class=\"reference\"><a href=\"#cite_note-international_Journal_of_Aviation_Psychology-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Health_care\">Health care</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=16\" title=\"Edit section: Health care\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><a href=\"https://wikipedia.com/wiki/Clinical_decision_support_system\" title=\"Clinical decision support system\">Clinical decision support systems</a> (CDSS) are designed to aid clinical decision-making. They have the potential to effect a great improvement in this regard, and to result in improved patient outcomes. Yet while CDSS, when used properly, bring about an overall improvement in performance, they also cause errors that may not be recognized owing to automation bias. One danger is that the incorrect advice given by these systems may cause users to change a correct decision that they have made on their own. Given the highly serious nature of some of the potential consequences of AB in the health-care field, it is especially important to be aware of this problem when it occurs in clinical settings.<sup id=\"cite_ref-goddard2012_13-10\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup>\n</p><p>Sometimes automation bias in clinical settings is a major problem that renders CDSS, on balance, counterproductive; sometimes it is  minor problem, with the benefits outweighing the damage done. One study found more automation bias among older users, but it was noted that could be a result not of age but of experience. Studies suggest, indeed, that familiarity with CDSS often leads to desensitization and habituation effects. Although automation bias occurs more often among persons who are inexperienced in a given task, inexperienced users exhibit the most performance improvement when they use CDSS. In one study, the use of CDSS improved clinicians' answers by 21%, from 29% to 50%, with 7% of correct non-CDSS answers being changed incorrectly.<sup id=\"cite_ref-goddard2012_13-11\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup>\n</p><p>A 2005 study found that when <a href=\"https://wikipedia.com/wiki/Primary_care_physician\" title=\"Primary care physician\">primary-care physicians</a> used electronic sources such as <a href=\"https://wikipedia.com/wiki/PubMed\" title=\"PubMed\">PubMed</a>, <a href=\"https://wikipedia.com/wiki/Medline\" class=\"mw-redirect\" title=\"Medline\">Medline</a>, and <a href=\"https://wikipedia.com/wiki/Google\" title=\"Google\">Google</a>, there was a \"small to medium\" increase in correct answers, while in an equally small percentage of instances the physicians were misled by their use of those sources, and changed correct to incorrect answers.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-13\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p><p>Studies in 2004 and 2008 that involved the effect of automated aids on diagnosis of <a href=\"https://wikipedia.com/wiki/Breast_cancer\" title=\"Breast cancer\">breast cancer</a> found clear evidence of automation bias involving omission errors. Cancers diagnosed in 46% of cases without automated aids were discovered in only 21% of cases with automated aids that failed to identify the cancer.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-14\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Military\">Military</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=17\" title=\"Edit section: Military\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Automation bias can be a crucial factor in the use of intelligent decision support systems for military command-and-control operations. One 2004 study found that automation bias effects have contributed to a number of fatal military decisions, including <a href=\"https://wikipedia.com/wiki/Friendly-fire\" class=\"mw-redirect\" title=\"Friendly-fire\">friendly-fire</a> killings during the <a href=\"https://wikipedia.com/wiki/Iraq_War\" title=\"Iraq War\">Iraq War</a>. Researchers have sought to determine the proper LOA for decision support systems in this field.<sup id=\"cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-15\" class=\"reference\"><a href=\"#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\">&#91;12&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Automotive\">Automotive</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=18\" title=\"Edit section: Automotive\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Automation complacency is also a challenge for automated driving systems in which the human only has to monitor the system or act as a fallback driver. This is for example discussed in the report of <a href=\"https://wikipedia.com/wiki/National_Transportation_Safety_Board\" title=\"National Transportation Safety Board\">National Transportation Safety Board</a> about the fatal accident between an UBER test vehicle and pedestrian <a href=\"https://wikipedia.com/wiki/Death_of_Elaine_Herzberg\" title=\"Death of Elaine Herzberg\">Elaine Herzberg</a>. <sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Correcting_bias\">Correcting bias</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=19\" title=\"Edit section: Correcting bias\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Automation bias can be mitigated by the design of automated systems, such as reducing the prominence of the display, decreasing detail or complexity of information displayed, or couching automated assistance as supportive information rather than as directives or commands.<sup id=\"cite_ref-goddard2012_13-12\" class=\"reference\"><a href=\"#cite_note-goddard2012-13\">&#91;13&#93;</a></sup> Training on an automated system, which includes introducing deliberate errors has been shown to be significantly more effective at reducing automation bias than just informing users that errors can occur.<sup id=\"cite_ref-bahner2008_21-0\" class=\"reference\"><a href=\"#cite_note-bahner2008-21\">&#91;21&#93;</a></sup> However, excessive checking and questioning automated assistance can increase time pressures and complexity of tasks thus reducing the benefits of automated assistance, so design of an automated decision support system can balance positive and negative effects rather than attempt to eliminate negative effects.<sup id=\"cite_ref-Alberdi2009_14-1\" class=\"reference\"><a href=\"#cite_note-Alberdi2009-14\">&#91;14&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=20\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"https://wikipedia.com/wiki/Algorithmic_bias\" title=\"Algorithmic bias\">Algorithmic bias</a></li>\n<li><a href=\"https://wikipedia.com/wiki/List_of_cognitive_biases\" title=\"List of cognitive biases\">List of cognitive biases</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Automation\" title=\"Automation\">Automation</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=21\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFCummings2004\" class=\"citation book cs1\">Cummings, Mary (2004). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20141101113133/http://web.mit.edu/aeroastro/labs/halab/papers/CummingsAIAAbias.pdf\">\"Automation Bias in Intelligent Time Critical Decision Support Systems\"</a> <span class=\"cs1-format\">(PDF)</span>. <a rel=\"nofollow\" class=\"external text\" href=\"http://web.mit.edu/aeroastro/labs/halab/papers/CummingsAIAAbias.pdf\"><i>AIAA 1st Intelligent Systems Technical Conference</i></a> <span class=\"cs1-format\">(PDF)</span>. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.2514%2F6.2004-6313\">10.2514/6.2004-6313</a>. <a href=\"https://wikipedia.com/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"https://wikipedia.com/wiki/Special:BookSources/978-1-62410-080-2\" title=\"Special:BookSources/978-1-62410-080-2\"><bdi>978-1-62410-080-2</bdi></a>. Archived from the original on 2014-11-01.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automation+Bias+in+Intelligent+Time+Critical+Decision+Support+Systems&amp;rft.btitle=AIAA+1st+Intelligent+Systems+Technical+Conference&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.2514%2F6.2004-6313&amp;rft.isbn=978-1-62410-080-2&amp;rft.aulast=Cummings&amp;rft.aufirst=Mary&amp;rft_id=http%3A%2F%2Fwayback.archive.org%2Fweb%2F20141101113133%2Fhttp%3A%2F%2Fweb.mit.edu%2Faeroastro%2Flabs%2Fhalab%2Fpapers%2FCummingsAIAAbias.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\">CS1 maint: BOT: original-url status unknown (<a href=\"https://wikipedia.com/wiki/Category:CS1_maint:_BOT:_original-url_status_unknown\" title=\"Category:CS1 maint: BOT: original-url status unknown\">link</a>)</span><style data-mw-deduplicate=\"TemplateStyles:r951705291\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\");background-image:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\");background-image:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\");background-image:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\");background-image:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\">Bruner, J. S., &amp; Tagiuri, R. 1954. \"The perception of people\". In G. Lindzey (Ed.), <i><a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/?id=zO63AAAAIAAJ&amp;q=+The+perception+of+people\">Handbook of social psychology (vol 2)</a>:</i> 634-654. Reading, MA: Addison-Wesley.</span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFMadhavanWiegmann2007\" class=\"citation journal cs1\">Madhavan, P.; Wiegmann, D. A. (2007-07-01). \"Similarities and differences between human–human and human–automation trust: an integrative review\". <i>Theoretical Issues in Ergonomics Science</i>. <b>8</b> (4): 277–301. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1080%2F14639220500337708\">10.1080/14639220500337708</a>. <a href=\"https://wikipedia.com/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:39064140\">39064140</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Theoretical+Issues+in+Ergonomics+Science&amp;rft.atitle=Similarities+and+differences+between+human%E2%80%93human+and+human%E2%80%93automation+trust%3A+an+integrative+review&amp;rft.volume=8&amp;rft.issue=4&amp;rft.pages=277-301&amp;rft.date=2007-07-01&amp;rft_id=info%3Adoi%2F10.1080%2F14639220500337708&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A39064140&amp;rft.aulast=Madhavan&amp;rft.aufirst=P.&amp;rft.au=Wiegmann%2C+D.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFDzindoletPetersonPomrankyPierce2003\" class=\"citation journal cs1\">Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P. (2003). \"The role of trust in automation reliance\". <i>International Journal of Human-Computer Studies</i>. <b>58</b> (6): 697–718. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2FS1071-5819%2803%2900038-7\">10.1016/S1071-5819(03)00038-7</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Human-Computer+Studies&amp;rft.atitle=The+role+of+trust+in+automation+reliance&amp;rft.volume=58&amp;rft.issue=6&amp;rft.pages=697-718&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1016%2FS1071-5819%2803%2900038-7&amp;rft.aulast=Dzindolet&amp;rft.aufirst=Mary+T.&amp;rft.au=Peterson%2C+Scott+A.&amp;rft.au=Pomranky%2C+Regina+A.&amp;rft.au=Pierce%2C+Linda+G.&amp;rft.au=Beck%2C+Hall+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-University_of_Illinois_at_Chicago-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-University_of_Illinois_at_Chicago_5-6\"><sup><i><b>g</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFSkitka\" class=\"citation web cs1\">Skitka, Linda. <a rel=\"nofollow\" class=\"external text\" href=\"https://lskitka.people.uic.edu/styled-7/styled-14/index.html\">\"Automation\"</a>. <i>University of Illinois</i>. University of Illinois at Chicago<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=University+of+Illinois&amp;rft.atitle=Automation&amp;rft.aulast=Skitka&amp;rft.aufirst=Linda&amp;rft_id=https%3A%2F%2Flskitka.people.uic.edu%2Fstyled-7%2Fstyled-14%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-ParasuramanRiley1997-6\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-ParasuramanRiley1997_6-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-ParasuramanRiley1997_6-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-ParasuramanRiley1997_6-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFParasuramanRiley1997\" class=\"citation journal cs1\">Parasuraman, Raja; Riley, Victor (1997). \"Humans and Automation: Use, Misuse, Disuse, Abuse\". <i>Human Factors: The Journal of the Human Factors and Ergonomics Society</i>. <b>39</b> (2): 230–253. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1518%2F001872097778543886\">10.1518/001872097778543886</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Human+Factors%3A+The+Journal+of+the+Human+Factors+and+Ergonomics+Society&amp;rft.atitle=Humans+and+Automation%3A+Use%2C+Misuse%2C+Disuse%2C+Abuse&amp;rft.volume=39&amp;rft.issue=2&amp;rft.pages=230-253&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1518%2F001872097778543886&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Riley%2C+Victor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-international_Journal_of_Aviation_Psychology-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-international_Journal_of_Aviation_Psychology_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-international_Journal_of_Aviation_Psychology_7-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-international_Journal_of_Aviation_Psychology_7-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-international_Journal_of_Aviation_Psychology_7-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFMosierSkitkaHeersBurdick1997\" class=\"citation journal cs1\">Mosier, Kathleen; Skitka, Linda; Heers, Susan; Burdick, Mark (1997). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/11805395\">\"Automation Bias: Decision Making and Performance in High-Tech Cockpits\"</a>. <i>International Journal of Aviation Psychology</i>. <b>8</b> (1): 47–63. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1207%2Fs15327108ijap0801_3\">10.1207/s15327108ijap0801_3</a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/11540946\">11540946</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Aviation+Psychology&amp;rft.atitle=Automation+Bias%3A+Decision+Making+and+Performance+in+High-Tech+Cockpits&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=47-63&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1207%2Fs15327108ijap0801_3&amp;rft_id=info%3Apmid%2F11540946&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen&amp;rft.au=Skitka%2C+Linda&amp;rft.au=Heers%2C+Susan&amp;rft.au=Burdick%2C+Mark&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F11805395&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-:0-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:0_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:0_8-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:0_8-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:0_8-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFLyellCoiera2016\" class=\"citation journal cs1\">Lyell, David; Coiera, Enrico (August 2016). <a rel=\"nofollow\" class=\"external text\" href=\"https://academic.oup.com/jamia/article/24/2/423/2631492/Automation-bias-and-verification-complexity-a\">\"Automation bias and verification complexity: a systematic review\"</a>. <i>Journal of the American Medical Informatics Association</i>. <b>24</b> (2): 424–431. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1093%2Fjamia%2Focw105\">10.1093/jamia/ocw105</a></span>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/27516495\">27516495</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Medical+Informatics+Association&amp;rft.atitle=Automation+bias+and+verification+complexity%3A+a+systematic+review&amp;rft.volume=24&amp;rft.issue=2&amp;rft.pages=424-431&amp;rft.date=2016-08&amp;rft_id=info%3Adoi%2F10.1093%2Fjamia%2Focw105&amp;rft_id=info%3Apmid%2F27516495&amp;rft.aulast=Lyell&amp;rft.aufirst=David&amp;rft.au=Coiera%2C+Enrico&amp;rft_id=https%3A%2F%2Facademic.oup.com%2Fjamia%2Farticle%2F24%2F2%2F423%2F2631492%2FAutomation-bias-and-verification-complexity-a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFTverskyKahneman1974\" class=\"citation journal cs1\">Tversky, A.; Kahneman, D. (1974). \"Judgment under Uncertainty: Heuristics and Biases\". <i>Science</i>. <b>185</b> (4157): 1124–1131. <a href=\"https://wikipedia.com/wiki/Bibcode_(identifier)\" class=\"mw-redirect\" title=\"Bibcode (identifier)\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/1974Sci...185.1124T\">1974Sci...185.1124T</a>. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1126%2Fscience.185.4157.1124\">10.1126/science.185.4157.1124</a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/17835457\">17835457</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Judgment+under+Uncertainty%3A+Heuristics+and+Biases&amp;rft.volume=185&amp;rft.issue=4157&amp;rft.pages=1124-1131&amp;rft.date=1974&amp;rft_id=info%3Apmid%2F17835457&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.185.4157.1124&amp;rft_id=info%3Abibcode%2F1974Sci...185.1124T&amp;rft.aulast=Tversky&amp;rft.aufirst=A.&amp;rft.au=Kahneman%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-Wickens2015-10\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Wickens2015_10-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Wickens2015_10-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFWickensHollandsBanburyParasuraman2015\" class=\"citation book cs1\">Wickens, Christopher D.; Hollands, Justin G.; Banbury, Simon; Parasuraman, Raja (2015). <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=_rFmCgAAQBAJ\"><i>Engineering Psychology and Human Performance</i></a> (4th ed.). Psychology Press. <a href=\"https://wikipedia.com/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"https://wikipedia.com/wiki/Special:BookSources/9781317351320\" title=\"Special:BookSources/9781317351320\"><bdi>9781317351320</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Engineering+Psychology+and+Human+Performance&amp;rft.edition=4th&amp;rft.pub=Psychology+Press&amp;rft.date=2015&amp;rft.isbn=9781317351320&amp;rft.aulast=Wickens&amp;rft.aufirst=Christopher+D.&amp;rft.au=Hollands%2C+Justin+G.&amp;rft.au=Banbury%2C+Simon&amp;rft.au=Parasuraman%2C+Raja&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D_rFmCgAAQBAJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-Sagepub-11\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Sagepub_11-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Sagepub_11-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Sagepub_11-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFMosierDunbarMcDonnellSkitka1998\" class=\"citation journal cs1\">Mosier, Kathleen L.; Dunbar, Melisa; McDonnell, Lori; Skitka, Linda J.; Burdick, Mark; Rosenblatt, Bonnie (1998). \"Automation Bias and Errors: Are Teams Better than Individuals?\". <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i>. <b>42</b> (3): 201–205. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1177%2F154193129804200304\">10.1177/154193129804200304</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Human+Factors+and+Ergonomics+Society+Annual+Meeting&amp;rft.atitle=Automation+Bias+and+Errors%3A+Are+Teams+Better+than+Individuals%3F&amp;rft.volume=42&amp;rft.issue=3&amp;rft.pages=201-205&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1177%2F154193129804200304&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen+L.&amp;rft.au=Dunbar%2C+Melisa&amp;rft.au=McDonnell%2C+Lori&amp;rft.au=Skitka%2C+Linda+J.&amp;rft.au=Burdick%2C+Mark&amp;rft.au=Rosenblatt%2C+Bonnie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-11\"><sup><i><b>l</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-12\"><sup><i><b>m</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-13\"><sup><i><b>n</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-14\"><sup><i><b>o</b></i></sup></a> <a href=\"#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-15\"><sup><i><b>p</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFParasuramanManzey2010\" class=\"citation journal cs1\">Parasuraman, Raja; Manzey, Dietrich (June 2010). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/47792928\">\"Complacency and Bias in Human Use of Automation: An Attentional Integration\"</a>. <i>The Journal of the Human Factors and Ergonomics Society</i>. <b>52</b> (3): 381–410. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1177%2F0018720810376055\">10.1177/0018720810376055</a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/21077562\">21077562</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">17 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+the+Human+Factors+and+Ergonomics+Society&amp;rft.atitle=Complacency+and+Bias+in+Human+Use+of+Automation%3A+An+Attentional+Integration&amp;rft.volume=52&amp;rft.issue=3&amp;rft.pages=381-410&amp;rft.date=2010-06&amp;rft_id=info%3Adoi%2F10.1177%2F0018720810376055&amp;rft_id=info%3Apmid%2F21077562&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Manzey%2C+Dietrich&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F47792928&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-goddard2012-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-goddard2012_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-11\"><sup><i><b>l</b></i></sup></a> <a href=\"#cite_ref-goddard2012_13-12\"><sup><i><b>m</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFGoddardRoudsariWyatt2012\" class=\"citation journal cs1\">Goddard, K.; Roudsari, A.; Wyatt, J. C. (2012). <a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//www.ncbi.nlm.nih.gov/pmc/articles/PMC3240751\">\"Automation bias: a systematic review of frequency, effect mediators, and mitigators\"</a>. <i>Journal of the American Medical Informatics Association</i>. <b>19</b> (1): 121–127. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1136%2Famiajnl-2011-000089\">10.1136/amiajnl-2011-000089</a>. <a href=\"https://wikipedia.com/wiki/PMC_(identifier)\" class=\"mw-redirect\" title=\"PMC (identifier)\">PMC</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//www.ncbi.nlm.nih.gov/pmc/articles/PMC3240751\">3240751</a></span>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/21685142\">21685142</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Medical+Informatics+Association&amp;rft.atitle=Automation+bias%3A+a+systematic+review+of+frequency%2C+effect+mediators%2C+and+mitigators&amp;rft.volume=19&amp;rft.issue=1&amp;rft.pages=121-127&amp;rft.date=2012&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3240751&amp;rft_id=info%3Apmid%2F21685142&amp;rft_id=info%3Adoi%2F10.1136%2Famiajnl-2011-000089&amp;rft.aulast=Goddard&amp;rft.aufirst=K.&amp;rft.au=Roudsari%2C+A.&amp;rft.au=Wyatt%2C+J.+C.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3240751&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-Alberdi2009-14\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Alberdi2009_14-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Alberdi2009_14-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFAlberdiStriginiPovyakaloAyton2009\" class=\"citation book cs1\">Alberdi, Eugenio; Strigini, Lorenzo; Povyakalo, Andrey A.; Ayton, Peter (2009). <a rel=\"nofollow\" class=\"external text\" href=\"http://openaccess.city.ac.uk/384/2/Alberdi-et-alSAFECOMP09.pdf\">\"Why Are People's Decisions Sometimes Worse with Computer Support?\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Computer Safety, Reliability, and Security</i>. Lecture Notes in Computer Science. <b>5775</b>. Springer Berlin Heidelberg. pp.&#160;18–31. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-04468-7_3\">10.1007/978-3-642-04468-7_3</a>. <a href=\"https://wikipedia.com/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"https://wikipedia.com/wiki/Special:BookSources/978-3-642-04467-0\" title=\"Special:BookSources/978-3-642-04467-0\"><bdi>978-3-642-04467-0</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Why+Are+People%27s+Decisions+Sometimes+Worse+with+Computer+Support%3F&amp;rft.btitle=Computer+Safety%2C+Reliability%2C+and+Security&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=18-31&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-04468-7_3&amp;rft.isbn=978-3-642-04467-0&amp;rft.aulast=Alberdi&amp;rft.aufirst=Eugenio&amp;rft.au=Strigini%2C+Lorenzo&amp;rft.au=Povyakalo%2C+Andrey+A.&amp;rft.au=Ayton%2C+Peter&amp;rft_id=http%3A%2F%2Fopenaccess.city.ac.uk%2F384%2F2%2FAlberdi-et-alSAFECOMP09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-goddard2014-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-goddard2014_15-0\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFGoddardRoudsariWyatt2014\" class=\"citation journal cs1\">Goddard, Kate; Roudsari, Abdul; Wyatt, Jeremy C. (2014). \"Automation bias: Empirical results assessing influencing factors\". <i>International Journal of Medical Informatics</i>. <b>83</b> (5): 368–375. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ijmedinf.2014.01.001\">10.1016/j.ijmedinf.2014.01.001</a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/24581700\">24581700</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Medical+Informatics&amp;rft.atitle=Automation+bias%3A+Empirical+results+assessing+influencing+factors&amp;rft.volume=83&amp;rft.issue=5&amp;rft.pages=368-375&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijmedinf.2014.01.001&amp;rft_id=info%3Apmid%2F24581700&amp;rft.aulast=Goddard&amp;rft.aufirst=Kate&amp;rft.au=Roudsari%2C+Abdul&amp;rft.au=Wyatt%2C+Jeremy+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-The_International_Journal_of_Aviation_Psychology-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-The_International_Journal_of_Aviation_Psychology_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-The_International_Journal_of_Aviation_Psychology_16-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite id=\"CITEREFMosierSkitkaDunbarMcDonnell2009\" class=\"citation journal cs1\">Mosier, Kathleen; Skitka, Linda; Dunbar, Melisa; McDonnell, Lori (November 13, 2009). \"Aircrews and Automation Bias: The Advantages of Teamwork?\". <i>The International Journal of Aviation Psychology</i>. <b>11</b> (1): 1–14. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1207%2FS15327108IJAP1101_1\">10.1207/S15327108IJAP1101_1</a>. <a href=\"https://wikipedia.com/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:4132245\">4132245</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Aviation+Psychology&amp;rft.atitle=Aircrews+and+Automation+Bias%3A+The+Advantages+of+Teamwork%3F&amp;rft.volume=11&amp;rft.issue=1&amp;rft.pages=1-14&amp;rft.date=2009-11-13&amp;rft_id=info%3Adoi%2F10.1207%2FS15327108IJAP1101_1&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A4132245&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen&amp;rft.au=Skitka%2C+Linda&amp;rft.au=Dunbar%2C+Melisa&amp;rft.au=McDonnell%2C+Lori&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFEndsley2017\" class=\"citation journal cs1\">Endsley, Mica (2017). \"From Here to Autonomy Lessons Learned From Human–Automation Research\". <i>Human Factors</i>. <b>59</b> (1): 5–27. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1177%2F0018720816681350\">10.1177/0018720816681350</a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/28146676\">28146676</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Human+Factors&amp;rft.atitle=From+Here+to+Autonomy+Lessons+Learned+From+Human%E2%80%93Automation+Research&amp;rft.volume=59&amp;rft.issue=1&amp;rft.pages=5-27&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.1177%2F0018720816681350&amp;rft_id=info%3Apmid%2F28146676&amp;rft.aulast=Endsley&amp;rft.aufirst=Mica&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFBainbridge1983\" class=\"citation journal cs1\"><a href=\"https://wikipedia.com/wiki/Lisanne_Bainbridge\" class=\"mw-redirect\" title=\"Lisanne Bainbridge\">Bainbridge, Lisanne</a> (1983). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf\">\"Ironies of automation\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Automatica</i>. <b>19</b> (6): 775–779. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2F0005-1098%2883%2990046-8\">10.1016/0005-1098(83)90046-8</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Automatica&amp;rft.atitle=Ironies+of+automation&amp;rft.volume=19&amp;rft.issue=6&amp;rft.pages=775-779&amp;rft.date=1983&amp;rft_id=info%3Adoi%2F10.1016%2F0005-1098%2883%2990046-8&amp;rft.aulast=Bainbridge&amp;rft.aufirst=Lisanne&amp;rft_id=https%3A%2F%2Fwww.ise.ncsu.edu%2Fwp-content%2Fuploads%2F2017%2F02%2FBainbridge_1983_Automatica.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-ParasuramanMolloy1993-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ParasuramanMolloy1993_19-0\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFParasuramanMolloySingh1993\" class=\"citation journal cs1\">Parasuraman, Raja; Molloy, Robert; Singh, Indramani L. (1993). \"Performance Consequences of Automation-Induced 'Complacency<span class=\"cs1-kern-right\">'</span>\". <i>The International Journal of Aviation Psychology</i>. <b>3</b>: 1–23. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1207%2Fs15327108ijap0301_1\">10.1207/s15327108ijap0301_1</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Aviation+Psychology&amp;rft.atitle=Performance+Consequences+of+Automation-Induced+%27Complacency%27&amp;rft.volume=3&amp;rft.pages=1-23&amp;rft.date=1993&amp;rft_id=info%3Adoi%2F10.1207%2Fs15327108ijap0301_1&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Molloy%2C+Robert&amp;rft.au=Singh%2C+Indramani+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.ntsb.gov/investigations/AccidentReports/Pages/HAR1903.aspx\">\"Collision Between Vehicle Controlled by Developmental Automated Driving System and Pedestrian\"</a>. <i>www.ntsb.gov</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">December 19,</span> 2019</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.ntsb.gov&amp;rft.atitle=Collision+Between+Vehicle+Controlled+by+Developmental+Automated+Driving+System+and+Pedestrian&amp;rft_id=https%3A%2F%2Fwww.ntsb.gov%2Finvestigations%2FAccidentReports%2FPages%2FHAR1903.aspx&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n<li id=\"cite_note-bahner2008-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-bahner2008_21-0\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFBahnerHüperManzey2008\" class=\"citation journal cs1\">Bahner, J. Elin; Hüper, Anke-Dorothea; Manzey, Dietrich (2008). \"Misuse of automated decision aids: Complacency, automation bias and the impact of training experience\". <i>International Journal of Human-Computer Studies</i>. <b>66</b> (9): 688–699. <a href=\"https://wikipedia.com/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ijhcs.2008.06.001\">10.1016/j.ijhcs.2008.06.001</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Human-Computer+Studies&amp;rft.atitle=Misuse+of+automated+decision+aids%3A+Complacency%2C+automation+bias+and+the+impact+of+training+experience&amp;rft.volume=66&amp;rft.issue=9&amp;rft.pages=688-699&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijhcs.2008.06.001&amp;rft.aulast=Bahner&amp;rft.aufirst=J.+Elin&amp;rft.au=H%C3%BCper%2C+Anke-Dorothea&amp;rft.au=Manzey%2C+Dietrich&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"Further_reading\">Further reading</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=22\" title=\"Edit section: Further reading\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><cite id=\"CITEREFGoddardRoudsariWyatt2011\" class=\"citation book cs1\">Goddard, K; Roudsari, A; Wyatt, J. C. (2011). <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=NsbaN_fXRe4C&amp;pg=PA17\">\"Automation bias - a hidden issue for clinical decision support system use\"</a>. <i>International Perspectives in Health Informatics</i>. Studies in Health Technology and Informatics. <b>164</b>. pp.&#160;17–22. <a href=\"https://wikipedia.com/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"https://wikipedia.com/wiki/Special:BookSources/978-1-60750-708-6\" title=\"Special:BookSources/978-1-60750-708-6\"><bdi>978-1-60750-708-6</bdi></a>. <a href=\"https://wikipedia.com/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://wikipedia.com//pubmed.ncbi.nlm.nih.gov/21335682\">21335682</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automation+bias+-+a+hidden+issue+for+clinical+decision+support+system+use&amp;rft.btitle=International+Perspectives+in+Health+Informatics&amp;rft.series=Studies+in+Health+Technology+and+Informatics&amp;rft.pages=17-22&amp;rft.date=2011&amp;rft_id=info%3Apmid%2F21335682&amp;rft.isbn=978-1-60750-708-6&amp;rft.aulast=Goddard&amp;rft.aufirst=K&amp;rft.au=Roudsari%2C+A&amp;rft.au=Wyatt%2C+J.+C.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DNsbaN_fXRe4C%26pg%3DPA17&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r951705291\"/></li></ul>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"https://wikipedia.com/w/index.php?title=Automation_bias&amp;action=edit&amp;section=23\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.forbes.com/sites/brucekasanoff/2017/03/29/sorry-you-cant-make-a-logical-data-driven-decision-without-intuition/\"><i>175 Reasons Why You Don't Think Clearly</i></a></li></ul>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Biases\" style=\"padding:3px\"><table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"https://wikipedia.com/wiki/Template:Biases\" title=\"Template:Biases\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"https://wikipedia.com/wiki/Template_talk:Biases\" title=\"Template talk:Biases\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Biases&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Biases\" style=\"font-size:114%;margin:0 4em\"><a href=\"https://wikipedia.com/wiki/Bias\" title=\"Bias\">Biases</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"https://wikipedia.com/wiki/Cognitive_bias\" title=\"Cognitive bias\">Cognitive biases</a></div></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Actor%E2%80%93observer_asymmetry\" title=\"Actor–observer asymmetry\">Actor–observer</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Acquiescence_bias\" title=\"Acquiescence bias\">Acquiescence</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Ambiguity_effect\" title=\"Ambiguity effect\">Ambiguity</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Anchoring\" class=\"mw-redirect\" title=\"Anchoring\">Anchoring</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Attentional_bias\" title=\"Attentional bias\">Attentional</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Attribution_bias\" title=\"Attribution bias\">Attribution</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Authority_bias\" title=\"Authority bias\">Authority</a></li>\n<li><a class=\"mw-selflink selflink\">Automation</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Belief_bias\" title=\"Belief bias\">Belief</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Bias_blind_spot\" title=\"Bias blind spot\">Blind spot</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Choice-supportive_bias\" title=\"Choice-supportive bias\">Choice-supportive</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Confirmation_bias\" title=\"Confirmation bias\">Confirmation</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Congruence_bias\" title=\"Congruence bias\">Congruence</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Cultural_bias\" title=\"Cultural bias\">Cultural</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Distinction_bias\" title=\"Distinction bias\">Distinction</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Dunning%E2%80%93Kruger_effect\" title=\"Dunning–Kruger effect\">Dunning–Kruger</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Egocentric_bias\" title=\"Egocentric bias\">Egocentric</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Emotional_bias\" title=\"Emotional bias\">Emotional</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Extrinsic_incentives_bias\" title=\"Extrinsic incentives bias\">Extrinsic incentives</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Fading_affect_bias\" title=\"Fading affect bias\">Fading affect</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Framing_effect_(psychology)\" title=\"Framing effect (psychology)\">Framing</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Fundamental_attribution_error\" title=\"Fundamental attribution error\">Correspondence</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Halo_effect\" title=\"Halo effect\">Halo effect</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Hindsight_bias\" title=\"Hindsight bias\">Hindsight</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Horn_effect\" title=\"Horn effect\">Horn effect</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Hostile_attribution_bias\" title=\"Hostile attribution bias\">Hostile attribution</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Impact_bias\" title=\"Impact bias\">Impact</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Implicit_stereotype\" title=\"Implicit stereotype\">Implicit</a></li>\n<li><a href=\"https://wikipedia.com/wiki/In-group_favoritism\" title=\"In-group favoritism\">In-group</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Mere-exposure_effect\" title=\"Mere-exposure effect\">Mere-exposure effect</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Negativity_bias\" title=\"Negativity bias\">Negativity</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Normalcy_bias\" title=\"Normalcy bias\">Normalcy</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Omission_bias\" title=\"Omission bias\">Omission</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Optimism_bias\" title=\"Optimism bias\">Optimism</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Out-group_homogeneity\" title=\"Out-group homogeneity\">Out-group homogeneity</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Outcome_bias\" title=\"Outcome bias\">Outcome</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Overton_window\" title=\"Overton window\">Overton window</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Precision_bias\" title=\"Precision bias\">Precision</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Present_bias\" title=\"Present bias\">Present</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Pro-innovation_bias\" title=\"Pro-innovation bias\">Pro-innovation</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Response_bias\" title=\"Response bias\">Response</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Restraint_bias\" title=\"Restraint bias\">Restraint</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Self-serving_bias\" title=\"Self-serving bias\">Self-serving</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Social_comparison_bias\" title=\"Social comparison bias\">Social comparison</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Status_quo_bias\" title=\"Status quo bias\">Status quo</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Time-saving_bias\" title=\"Time-saving bias\">Time-saving</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Trait_ascription_bias\" title=\"Trait ascription bias\">Trait ascription</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Von_Restorff_effect\" title=\"Von Restorff effect\">von Restorff effect</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Zero-risk_bias\" title=\"Zero-risk bias\">Zero-risk</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Cognitive_bias_in_animals\" title=\"Cognitive bias in animals\">In animals</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"https://wikipedia.com/wiki/Bias_(statistics)\" title=\"Bias (statistics)\">Statistical biases</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Bias_of_an_estimator\" title=\"Bias of an estimator\">Estimator</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Forecast_bias\" title=\"Forecast bias\">Forecast</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Healthy_user_bias\" title=\"Healthy user bias\">Healthy user</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Information_bias_(epidemiology)\" title=\"Information bias (epidemiology)\">Information</a>\n<ul><li><a href=\"https://wikipedia.com/wiki/Information_bias_(psychology)\" title=\"Information bias (psychology)\">Psychological</a></li></ul></li>\n<li><a href=\"https://wikipedia.com/wiki/Lead_time_bias\" title=\"Lead time bias\">Lead time</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Length_time_bias\" title=\"Length time bias\">Length time</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Participation_bias\" title=\"Participation bias\">Non-response</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Observer_bias\" title=\"Observer bias\">Observer</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Omitted-variable_bias\" title=\"Omitted-variable bias\">Omitted-variable</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Participation_bias\" title=\"Participation bias\">Participation</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Recall_bias\" title=\"Recall bias\">Recall</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Sampling_bias\" title=\"Sampling bias\">Sampling</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Selection_bias\" title=\"Selection bias\">Selection</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Self-selection_bias\" title=\"Self-selection bias\">Self-selection</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Social_desirability_bias\" title=\"Social desirability bias\">Social desirability</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Spectrum_bias\" title=\"Spectrum bias\">Spectrum</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Survivorship_bias\" title=\"Survivorship bias\">Survivorship</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Systematic_error\" class=\"mw-redirect\" title=\"Systematic error\">Systematic error</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Systemic_bias\" title=\"Systemic bias\">Systemic</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Verification_bias\" title=\"Verification bias\">Verification</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Wet_bias\" title=\"Wet bias\">Wet</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Other biases</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Academic_bias\" title=\"Academic bias\">Academic</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Funding_bias\" title=\"Funding bias\">Funding</a></li>\n<li><a href=\"https://wikipedia.com/wiki/FUTON_bias\" title=\"FUTON bias\">FUTON</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Inductive_bias\" title=\"Inductive bias\">Inductive</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Infrastructure_bias\" title=\"Infrastructure bias\">Infrastructure</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Inherent_bias\" title=\"Inherent bias\">Inherent</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Bias_in_education\" title=\"Bias in education\">In education</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_bias\" title=\"Media bias\">Media</a>\n<ul><li><a href=\"https://wikipedia.com/wiki/False_balance\" title=\"False balance\">False balance</a></li>\n<li><a href=\"https://wikipedia.com/wiki/United_States_news_media_and_the_Vietnam_War\" title=\"United States news media and the Vietnam War\">Vietnam War</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_of_Norway\" class=\"mw-redirect\" title=\"Media of Norway\">Norway</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_bias_in_South_Asia\" title=\"Media bias in South Asia\">South Asia</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_of_Sweden\" class=\"mw-redirect\" title=\"Media of Sweden\">Sweden</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_bias_in_the_United_States\" title=\"Media bias in the United States\">United States</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_coverage_of_the_Arab%E2%80%93Israeli_conflict\" title=\"Media coverage of the Arab–Israeli conflict\">Arab–Israeli conflict</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Media_portrayal_of_the_Ukrainian_crisis\" title=\"Media portrayal of the Ukrainian crisis\">Ukraine</a></li></ul></li>\n<li><a href=\"https://wikipedia.com/wiki/Net_bias\" title=\"Net bias\">Net</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Political_bias\" title=\"Political bias\">Political bias</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Publication_bias\" title=\"Publication bias\">Publication</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Reporting_bias\" title=\"Reporting bias\">Reporting</a></li>\n<li><a href=\"https://wikipedia.com/wiki/White_hat_bias\" title=\"White hat bias\">White hat</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Bias reduction</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"https://wikipedia.com/wiki/Cognitive_bias_mitigation\" title=\"Cognitive bias mitigation\">Cognitive bias mitigation</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Debiasing\" title=\"Debiasing\">Debiasing</a></li>\n<li><a href=\"https://wikipedia.com/wiki/Heuristics_in_judgment_and_decision-making\" title=\"Heuristics in judgment and decision-making\">Heuristics in judgment and decision-making</a></li></ul>\n</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div>Lists: <a href=\"https://wikipedia.com/wiki/List_of_cognitive_biases\" title=\"List of cognitive biases\">General</a>&#160;<b>&#183;</b>&#32;<a href=\"https://wikipedia.com/wiki/List_of_memory_biases\" title=\"List of memory biases\">Memory</a></div></td></tr></tbody></table></div>\n\n\n\n\n</div>"}